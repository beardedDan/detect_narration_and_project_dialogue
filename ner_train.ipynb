{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b4f0bf5-fe96-4dc9-af85-3f51ef939085",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 23:43:48.699043: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-05 23:43:48.699068: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-05 23:43:48.699084: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The NER model module importing from directory  /media/daniel/HDD1/AI574/Project\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.load_library(\"/etc/alternatives/libcudnn_so\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import utils\n",
    "import ner_module\n",
    "from ner_module import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a38e8a89-2d17-4434-b2f1-32b60f296711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 2\n"
     ]
    }
   ],
   "source": [
    "cluster_spec = {\n",
    "    \"worker\": [\"worker1:port\", \"worker2:port\"],\n",
    "    \"chief\": [\"chief1:port\"]\n",
    "}\n",
    "cluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver(cluster_spec)\n",
    "strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(\n",
    "    communication=tf.distribute.experimental.CollectiveCommunication.AUTO,\n",
    "    cluster_resolver=cluster_resolver\n",
    ")\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "368fa4b4-9ace-45b6-94a5-7de311f6e302",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing preloaded training and validation data\n",
      "File /media/daniel/HDD1/AI574/Project/vocabulary.txt exists. Overwriting existing file.\n"
     ]
    }
   ],
   "source": [
    "ner_data_importer = ImportData(source=\"existing\")\n",
    "train_dataset, val_dataset, lookup_layer = ner_data_importer.import_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74c735fb-7762-4b0c-aea6-25ed271b5311",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = \"N\"\n",
    "if train == \"Y\":\n",
    "    train_model = TrainModel()\n",
    "    train_model.build_model()\n",
    "    train_model.compile_model()\n",
    "    history = train_model.train(train_data=train_dataset,\n",
    "                                val_data = val_dataset,\n",
    "                                epochs=500, batch_size=32)\n",
    "    train_model.save_model('NER_saved_recent')\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c50b23f-04a7-44b3-947b-46d5d3b71cd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ner_model = tf.keras.models.load_model('NER_saved_recent', custom_objects={'CustomNonPaddingTokenLoss': CustomNonPaddingTokenLoss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89d33a18-873d-43b3-9b1f-e061c1c52d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  988 10950   204   628     6  3938   215  5773    26  1036     0     1\n",
      "      0     0]], shape=(1, 14), dtype=int64)\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "def lowercase_and_convert_to_ids(tokens):\n",
    "    tokens = tf.strings.lower(tokens)\n",
    "    return lookup_layer(tokens)\n",
    "def tokenize_and_convert_to_ids(text):\n",
    "    tokens = text.split()\n",
    "    return lowercase_and_convert_to_ids(tokens)\n",
    "\n",
    "sample_input = tokenize_and_convert_to_ids(\n",
    "    \"eu rejects german call to boycott british lamb from Steve parson the funky Parson\"\n",
    ")\n",
    "sample_input = tf.reshape(sample_input, shape=[1, -1])\n",
    "print(sample_input)\n",
    "\n",
    "output = ner_model.predict(sample_input)\n",
    "prediction = np.argmax(output, axis=-1)[0]\n",
    "prediction = [MAPPING[i] for i in prediction]\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcdc2b3f-9445-4584-979e-be832ff848b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/daniel/HDD1/AI574/gutenberg/data/raw/PG10002_raw.txt\n",
      "/media/daniel/HDD1/AI574/gutenberg/data/raw/PG10005_raw.txt\n",
      "/media/daniel/HDD1/AI574/gutenberg/data/raw/PG10006_raw.txt\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/media/daniel/HDD1/AI574/gutenberg/data/raw'\n",
    "selected_works_path = '/media/daniel/HDD1/AI574/Project/selected_works_idx.csv'\n",
    "import csv\n",
    "import re\n",
    "cur_txt = ''\n",
    "with open(selected_works_path, mode='r', newline='') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    i = 0\n",
    "    for row in csv_reader:\n",
    "        if i == 3:\n",
    "            break\n",
    "        target_path = os.path.join('/media/daniel/HDD1/AI574/gutenberg/data/raw',row[0]+'_raw.txt')\n",
    "        print(target_path)\n",
    "        rows_as_strings = []\n",
    "        with open(target_path, mode='r', newline='') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "            for row in csv_reader:\n",
    "                row_string = ' '.join(row)\n",
    "                rows_as_strings.append(row_string)\n",
    "        cur_txt = '\\n'.join(rows_as_strings)                \n",
    "        cur_txt = utils.remove_special_characters(cur_txt, remove_digits=True).lower()\n",
    "\n",
    "        # print(len(cur_txt))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c8f64e0-a5aa-472f-a2f6-23ff36e9a3f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cur_txt = cur_txt[:20000]\n",
    "orig_words = cur_txt.split()\n",
    "# print(cur_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cde16634-04ae-4849-9353-542f32ef2a90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 97ms/step\n"
     ]
    }
   ],
   "source": [
    "sample_input = tokenize_and_convert_to_ids(cur_txt)\n",
    "sample_input = tf.reshape(sample_input, shape=[1, -1])\n",
    "# print(sample_input)\n",
    "\n",
    "output = ner_model.predict(sample_input)\n",
    "prediction = np.argmax(output, axis=-1)[0]\n",
    "prediction = [MAPPING[i] for i in prediction]\n",
    "# print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a61ec07e-5260-4b4e-9de5-2d3aa9830b22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knight', 'john', 'gore', 'james', 'god']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions = [index for index, value in enumerate(prediction) if value != 'O']\n",
    "NER_words = list(set([orig_words[index] for index in positions]))\n",
    "NER_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "922ef394-8710-46e8-9c7d-20f00192285f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['robert',\n",
       " 'garvindavemorgan',\n",
       " 'gore',\n",
       " 'giovanni',\n",
       " 'joanna',\n",
       " 'johnaddington',\n",
       " 'james',\n",
       " 'trojan']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.named_persons_w_spacy(cur_txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
