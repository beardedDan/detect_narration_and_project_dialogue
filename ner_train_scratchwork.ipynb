{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b4f0bf5-fe96-4dc9-af85-3f51ef939085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "from tensorflow import keras\n",
    "tf.load_library(\"/etc/alternatives/libcudnn_so\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "# import ner_module\n",
    "# from ner_module import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22c39599-4cd4-40ea-b04d-9df75dbbaa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 64\n",
    "class ImportData():\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)        \n",
    "            \n",
    "    def export_to_file(self,export_file_path, data):\n",
    "        export_file_path_features = export_file_path + \"_features.txt\"\n",
    "        export_file_path_labels = export_file_path + \"_labels.txt\"\n",
    "        with open(export_file_path_features, \"w\") as f:\n",
    "            for record in data:\n",
    "                ner_tags = record[\"ner_tags\"]\n",
    "                tokens = record[\"tokens\"]\n",
    "                if len(tokens) > 0:\n",
    "                    binary_tags = self.per_tags(ner_tags)\n",
    "                    f.write(\n",
    "                          \"\\t\".join(tokens)\n",
    "                        + \"\\n\"\n",
    "                    )\n",
    "\n",
    "        with open(export_file_path_labels, \"w\") as f:\n",
    "            for record in data:\n",
    "                ner_tags = record[\"ner_tags\"]\n",
    "                tokens = record[\"tokens\"]\n",
    "                if len(tokens) > 0:\n",
    "                    binary_tags = self.per_tags(ner_tags)\n",
    "                    f.write(\n",
    "                          \"\\t\".join(map(str, binary_tags))\n",
    "                        + \"\\n\"\n",
    "                    )\n",
    "\n",
    "    def per_tags(self,nertags):\n",
    "        return [1 if x in (1,2) else 0 for x in nertags]\n",
    "\n",
    "    def write_vocab_to_file(self,file_path,vocab):\n",
    "        if not os.path.exists(file_path):\n",
    "            with open(file_path, 'w') as file:\n",
    "                file.write(\"This is a new file.\")\n",
    "            print(f\"File {file_path} created successfully.\")\n",
    "        else:\n",
    "            print(f\"File {file_path} exists. Overwriting existing file.\")\n",
    "\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            for item in vocab:\n",
    "                file.write(item+\"\\n\")\n",
    "\n",
    "    def write_training_and_validation_data_to_file(self,training,validation):\n",
    "        if os.path.exists(\"./data\"):\n",
    "            self.export_to_file(\"./data/train\", training)\n",
    "            self.export_to_file(\"./data/val\", validation)\n",
    "            print(\"training and validation data saved to \",os.path.join(current_directory,'data'))\n",
    "        else:\n",
    "            print(\"Creating directory for training and validation data: \",os.path.join(current_directory,'data'))\n",
    "            os.mkdir(\"data\")\n",
    "            self.export_to_file(\"./data/train\", training)\n",
    "            self.export_to_file(\"./data/val\", validation)\n",
    "            print(\"training and validation data saved to \",os.path.join(current_directory,'data'))\n",
    "\n",
    "    def import_data(self):\n",
    "        if self.source == \"conll\":\n",
    "            conll_data = load_dataset(\"conll2003\")\n",
    "            print(\"Importing conll data\")\n",
    "            self.write_training_and_validation_data_to_file(conll_data[\"train\"],conll_data[\"validation\"])\n",
    "            \n",
    "            with open('./data/train_features.txt', 'r') as f:\n",
    "                train_features = f.read()\n",
    "                \n",
    "            vocabulary = set()\n",
    "            for line in train_features:\n",
    "                split_line = line.split('\\t')\n",
    "                for item in split_line:\n",
    "                    vocabulary.update(item)\n",
    "            vocabulary = sorted(vocabulary)\n",
    "            with open('./data/vocabulary.txt', 'w') as vocab_file:\n",
    "                for word in vocabulary:\n",
    "                    vocab_file.write(word + '\\n')\n",
    "                \n",
    "                \n",
    "            with open('./data/train_labels.txt', 'r') as f:\n",
    "                train_labels = f.read()                  \n",
    "                \n",
    "            with open('./data/val_features.txt', 'r') as f:\n",
    "                val_features = f.read()\n",
    "            with open('./data/val_labels.txt', 'r') as f:\n",
    "                val_labels = f.read()                  \n",
    "                \n",
    "            train_features = train_features[:100]\n",
    "            train_labels = train_labels[:100]\n",
    "        return train_features, train_labels, val_features, val_labels, vocabulary\n",
    "\n",
    "\n",
    "class PreProcessData():\n",
    "    def __init__(self, vocab_size=20000,source='conll',**kwargs):    \n",
    "        self.vocab_size=vocab_size\n",
    "        self.source=source\n",
    "        # self.lookup_layer = tf.keras.layers.StringLookup(vocabulary=vocabulary)\n",
    "        # self.vocabulary = None    \n",
    "    \n",
    "   \n",
    "    def preprocess(self, features, labels):\n",
    "        all_input_ids = []\n",
    "        all_attention_mask = []\n",
    "        all_labels = []\n",
    "        data = {'features': features, 'labels': labels}\n",
    "        tokens = []\n",
    "        seen_tokens = set()        \n",
    "        data = list(zip(features.split('\\n'), labels.split('\\n')))\n",
    "        for line in data:\n",
    "            combine_line = list(zip(line[0].split('\\t'), line[1].split('\\t')))\n",
    "            for item in combine_line:\n",
    "                token_str = item[0].lower()\n",
    "                if token_str not in seen_tokens and item[1] != '':\n",
    "                    print(item)\n",
    "                    token = tokenizer(token_str, padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
    "                    print(\"TOKEN: \",token)\n",
    "                    label_matrix = np.zeros(2)\n",
    "                    label_matrix[int(item[1])] = 1\n",
    "                    print(\"LABEL MATRIX: \", label_matrix)\n",
    "                    all_input_ids.append(token['input_ids'])\n",
    "                    all_attention_mask.append(token['attention_mask'])\n",
    "                    all_labels.append(label_matrix.tolist())\n",
    "                    seen_tokens.add(token_str)\n",
    "        target_len = len(data)\n",
    "        \n",
    "        print(\"ALL LABELS: \",all_labels)\n",
    "        print(\"ALL INPUT IDS: \",all_input_ids)        \n",
    "        \n",
    "        all_input_ids = all_input_ids[:target_len] + [[0] * MAX_LEN] * (target_len - len(all_input_ids))\n",
    "        all_attention_mask = all_attention_mask[:target_len] + [[0] * MAX_LEN] * (target_len - len(all_attention_mask))\n",
    "        all_labels = all_labels[:target_len] + [[0, 0]] * (target_len - len(all_labels))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        return {\"input_ids\": all_input_ids[0], \"attention_mask\": all_attention_mask[0], \"labels\": all_labels[0]}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa4c372-10c4-49cb-ba2c-c9ec2aa7474e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = self.map_record_to_training_data(train_data)\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a38e8a89-2d17-4434-b2f1-32b60f296711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 2\n"
     ]
    }
   ],
   "source": [
    "cluster_spec = {\n",
    "    \"worker\": [\"worker1:port\", \"worker2:port\"],\n",
    "    \"chief\": [\"chief1:port\"]\n",
    "}\n",
    "cluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver(cluster_spec)\n",
    "strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(\n",
    "    communication=tf.distribute.experimental.CollectiveCommunication.AUTO,\n",
    "    cluster_resolver=cluster_resolver\n",
    ")\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "368fa4b4-9ace-45b6-94a5-7de311f6e302",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing conll data\n",
      "training and validation data saved to  /media/daniel/HDD1/AI574/Project/data\n"
     ]
    }
   ],
   "source": [
    "ner_data_importer = ImportData(source=\"conll\", vocab_size=100000)\n",
    "train_features, train_labels, val_features, val_labels, vocabulary = ner_data_importer.import_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "453a736f-5861-49f3-bece-16a08ffef859",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('EU', '0')\n",
      "TOKEN:  {'input_ids': [101, 7327, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "LABEL MATRIX:  [1. 0.]\n",
      "('rejects', '0')\n",
      "TOKEN:  {'input_ids': [101, 19164, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "LABEL MATRIX:  [1. 0.]\n",
      "('German', '0')\n",
      "TOKEN:  {'input_ids': [101, 2446, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "LABEL MATRIX:  [1. 0.]\n",
      "('call', '0')\n",
      "TOKEN:  {'input_ids': [101, 2655, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "LABEL MATRIX:  [1. 0.]\n",
      "('to', '0')\n",
      "TOKEN:  {'input_ids': [101, 2000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "LABEL MATRIX:  [1. 0.]\n",
      "('boycott', '0')\n",
      "TOKEN:  {'input_ids': [101, 17757, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "LABEL MATRIX:  [1. 0.]\n",
      "('British', '0')\n",
      "TOKEN:  {'input_ids': [101, 2329, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "LABEL MATRIX:  [1. 0.]\n",
      "('lamb', '0')\n",
      "TOKEN:  {'input_ids': [101, 12559, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "LABEL MATRIX:  [1. 0.]\n",
      "('.', '0')\n",
      "TOKEN:  {'input_ids': [101, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "LABEL MATRIX:  [1. 0.]\n",
      "('Peter', '1')\n",
      "TOKEN:  {'input_ids': [101, 2848, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "LABEL MATRIX:  [0. 1.]\n",
      "('Blackburn', '1')\n",
      "TOKEN:  {'input_ids': [101, 13934, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "LABEL MATRIX:  [0. 1.]\n",
      "('BRUSSELS', '0')\n",
      "TOKEN:  {'input_ids': [101, 9371, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "LABEL MATRIX:  [1. 0.]\n",
      "('1996-08-22', '0')\n",
      "TOKEN:  {'input_ids': [101, 2727, 1011, 5511, 1011, 2570, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "LABEL MATRIX:  [1. 0.]\n",
      "('The', '0')\n",
      "TOKEN:  {'input_ids': [101, 1996, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "LABEL MATRIX:  [1. 0.]\n",
      "('European', '0')\n",
      "TOKEN:  {'input_ids': [101, 2647, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "LABEL MATRIX:  [1. 0.]\n",
      "('Co', '0')\n",
      "TOKEN:  {'input_ids': [101, 2522, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "LABEL MATRIX:  [1. 0.]\n",
      "ALL LABELS:  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "ALL INPUT IDS:  [[101, 7327, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 19164, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2446, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2655, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 17757, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2329, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 12559, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2848, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 13934, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 9371, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2727, 1011, 5511, 1011, 2570, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1996, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2647, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2522, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "ner_data_preprocess = PreProcessData()\n",
    "train_dataset = ner_data_preprocess.preprocess(train_features, train_labels)\n",
    "# validation_dataset = ner_data_preprocess.preprocess(val_features, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73d7bec-393a-4da8-ae2e-50930dd625e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(self, features, labels):\n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "    all_labels = []\n",
    "    data = {'features': features, 'labels': labels}\n",
    "    tokens = []\n",
    "    seen_tokens = set()        \n",
    "    data = list(zip(features.split('\\n'), labels.split('\\n')))\n",
    "    for line in data:\n",
    "        combine_line = list(zip(line[0].split('\\t'), line[1].split('\\t')))\n",
    "        for item in combine_line:\n",
    "            token_str = item[0].lower()\n",
    "            if token_str not in seen_tokens and item[1] != '':\n",
    "                token = tokenizer(token_str, padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
    "                label_matrix = np.zeros(2)\n",
    "                label_matrix[int(item[1])] = 1\n",
    "                all_input_ids.append(token['input_ids'])\n",
    "                all_attention_mask.append(token['attention_mask'])\n",
    "                all_labels.append(label_matrix.tolist())\n",
    "                seen_tokens.add(token_str)\n",
    "    target_len = len(data)\n",
    "    all_input_ids = all_input_ids[:target_len] + [[0] * MAX_LEN] * (target_len - len(all_input_ids))\n",
    "    all_attention_mask = all_attention_mask[:target_len] + [[0] * MAX_LEN] * (target_len - len(all_attention_mask))\n",
    "    all_labels = all_labels[:target_len] + [[0, 0]] * (target_len - len(all_labels))\n",
    "\n",
    "    print(all_input_ids)\n",
    "    print(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25875fe-7f2c-4429-b79e-424071cb9248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think this is part of the solution:\n",
    "\n",
    "# X_train = list(zip(train_dataset[\"input_ids\"], train_dataset[\"attention_mask\"]))\n",
    "# y_train = [np.argmax(label) for label in train_dataset[\"labels\"]]\n",
    "# X_train_array = np.array(X_train)\n",
    "# X_train_flat = X_train_array.reshape(X_train_array.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2382115a-0433-4ad6-9919-cfbc56b6faee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa83f1cd-811f-47d5-915c-975393364198",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions 512 and 2 are not compatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset_tf \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m validation_dataset_tf \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((validation_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],validation_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:821\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# from_tensor_slices_op -> dataset_ops).\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_tensor_slices_op\n\u001b[0;32m--> 821\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_tensor_slices_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:25\u001b[0m, in \u001b[0;36m_from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_tensor_slices\u001b[39m(tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 25\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_TensorSliceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:45\u001b[0m, in \u001b[0;36m_TensorSliceDataset.__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m     42\u001b[0m batch_dim \u001b[38;5;241m=\u001b[39m tensor_shape\u001b[38;5;241m.\u001b[39mDimension(\n\u001b[1;32m     43\u001b[0m     tensor_shape\u001b[38;5;241m.\u001b[39mdimension_value(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_shape()[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m---> 45\u001b[0m   \u001b[43mbatch_dim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_is_compatible_with\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtensor_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDimension\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m          \u001b[49m\u001b[43mtensor_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdimension_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39mtensor_slice_dataset(\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors,\n\u001b[1;32m     51\u001b[0m     output_shapes\u001b[38;5;241m=\u001b[39mstructure\u001b[38;5;241m.\u001b[39mget_flat_tensor_shapes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure),\n\u001b[1;32m     52\u001b[0m     is_files\u001b[38;5;241m=\u001b[39mis_files,\n\u001b[1;32m     53\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\u001b[38;5;241m.\u001b[39mSerializeToString())\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(variant_tensor)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow/python/framework/tensor_shape.py:300\u001b[0m, in \u001b[0;36mDimension.assert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Raises an exception if `other` is not compatible with this Dimension.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m    is_compatible_with).\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_compatible_with(other):\n\u001b[0;32m--> 300\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimensions \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m are not compatible\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    301\u001b[0m                    (\u001b[38;5;28mself\u001b[39m, other))\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions 512 and 2 are not compatible"
     ]
    }
   ],
   "source": [
    "train_dataset_tf = tf.data.Dataset.from_tensor_slices((train_dataset[\"input_ids\"],train_dataset[\"labels\"]))\n",
    "validation_dataset_tf = tf.data.Dataset.from_tensor_slices((validation_dataset[\"input_ids\"],validation_dataset[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4eaf1d-3ddf-40c0-bbc6-3062879c2918",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_shuf = train_dataset_tf.shuffle(512).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "validation_dataset_shuf = validation_dataset_tf.shuffle(512).batch(64).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508e3f0a-0b19-46f3-a9fd-0cbc4961b97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "# train_dataset = (\n",
    "#     train_data.map(self.map_record_to_training_data)\n",
    "#     .map(lambda x, y: (self.lowercase_and_convert_to_ids(x), y))\n",
    "#     .padded_batch(batch_size)\n",
    "# )\n",
    "# val_dataset = (\n",
    "#     val_data.map(self.map_record_to_training_data)\n",
    "#     .map(lambda x, y: (self.lowercase_and_convert_to_ids(x), y))\n",
    "#     .padded_batch(batch_size)\n",
    "# )            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e52c98c-4213-442c-8fbd-10574d9dbe8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c735fb-7762-4b0c-aea6-25ed271b5311",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = \"Y\"\n",
    "if train == \"Y\":\n",
    "    train_model = TrainModel()\n",
    "    train_model.build_model()\n",
    "    train_model.compile_model()\n",
    "    history = train_model.train(train_data=train_dataset_shuf,\n",
    "                                val_data = validation_dataset_shuf,\n",
    "                                epochs=500, batch_size=32)\n",
    "    train_model.save_model('NER_saved_recent')\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe49c58-fb71-4967-96e4-def0ae6f1aea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b75d20-df6d-44d5-8c6b-dd23b6236028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67794cf7-87a8-4a75-93ff-ff431830c3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5af15ae-208d-4c9d-ac1f-f6fc71898a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8ffc185-db10-4506-99b6-e02aa5f4c5b3",
   "metadata": {},
   "source": [
    "# Everything below this applies tags to the original text using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaf9858-6497-4d72-9e32-03f6370afb20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb212c8-5b99-433c-adf0-3541b6951178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa0a581-f7c2-4df2-b457-3af8e6702d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c68f96-fb69-45e5-a977-7c6c3ee9d202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459e61f7-dd22-4e93-874f-776e1c47a080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08238345-6802-4c4e-9b88-8dc3b0698c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00fd9a1-7fee-41d4-a7b0-4b1d8c416a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba66e4d0-a782-408c-b524-e25017d03ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f335cc2-0c2f-4fa6-a2b6-d0b222a8cb22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fe0ac8-c76d-4b28-b9fa-6965f7dfd244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c50b23f-04a7-44b3-947b-46d5d3b71cd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ner_model = tf.keras.models.load_model('NER_saved_recent', custom_objects={'CustomNonPaddingTokenLoss': CustomNonPaddingTokenLoss})\n",
    "# def lowercase_and_convert_to_ids(tokens):\n",
    "#     tokens = tf.strings.lower(tokens)\n",
    "#     return lookup_layer(tokens)\n",
    "# def tokenize_and_convert_to_ids(text):\n",
    "#     tokens = text.split()\n",
    "#     return lowercase_and_convert_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d33a18-873d-43b3-9b1f-e061c1c52d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lowercase_and_convert_to_ids(tokens):\n",
    "#     tokens = tf.strings.lower(tokens)\n",
    "#     return lookup_layer(tokens)\n",
    "# def tokenize_and_convert_to_ids(text):\n",
    "#     tokens = text.split()\n",
    "#     return lowercase_and_convert_to_ids(tokens)\n",
    "\n",
    "# sample_input = tokenize_and_convert_to_ids(\n",
    "#     \"eu rejects german call to boycott british lamb from Steve parson the funky Parson\"\n",
    "# )\n",
    "# sample_input = tf.reshape(sample_input, shape=[1, -1])\n",
    "# print(sample_input)\n",
    "\n",
    "# output = ner_model.predict(sample_input)\n",
    "# prediction = np.argmax(output, axis=-1)[0]\n",
    "# prediction = [MAPPING[i] for i in prediction]\n",
    "# print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a160d72d-9141-41aa-b4e3-19008d6be821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "def group_consecutive_words(text_w_word_indicators):\n",
    "    grouped_words = []\n",
    "    for key, group in groupby(text_w_word_indicators, key=itemgetter(1)):\n",
    "        words = [word for word, _ in group]\n",
    "        grouped_words.append((words, key))\n",
    "    return grouped_words    \n",
    "\n",
    "def per_tree(text):\n",
    "    tree_str = \"\"\n",
    "    for group in text:\n",
    "        words, indicator = group\n",
    "        if indicator == 'B-PER': \n",
    "            quoted_group = ' '.join([f\"{word}\" for word in words])\n",
    "            tree_str += f\"(PER {quoted_group})\"\n",
    "        else:\n",
    "            tree_str += ' '.join(words) + \" \"       \n",
    "    tree_str = tree_str.strip()\n",
    "    return tree_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bb9bdd-9141-4fd1-af3d-3b23e464a441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# class QuotationIndicator():\n",
    "#     def __init__(self, num_threads=4, **kwargs):\n",
    "#         self.pattern = re.compile(r'(\".*?\")')\n",
    "#         self.num_threads = num_threads\n",
    "#         for key, value in kwargs.items():\n",
    "#             setattr(self, key, value) \n",
    "\n",
    "#     def process_chunk(self, chunk):\n",
    "#         matches = [(m.start(1), m.end(1)) for m in self.pattern.finditer(chunk)]\n",
    "#         words = chunk.split()\n",
    "#         indicators = [0] * len(words)\n",
    "#         word_start = 0\n",
    "#         for i, word in enumerate(words):\n",
    "#             word_end = word_start + len(word)\n",
    "#             if any(start + 1 <= word_start < end - 1 for start, end in matches):\n",
    "#                 indicators[i] = 1\n",
    "#             word_start = word_end + 1\n",
    "#         return indicators\n",
    "    \n",
    "#     def combine_text_and_indicators(self, text, indicators):\n",
    "#         text = text.split()\n",
    "#         word_indicators = list(zip(text, indicators))\n",
    "#         print(word_indicators)\n",
    "#         return word_indicators\n",
    "    \n",
    "#     def group_consecutive_words(self, text_w_word_indicators):\n",
    "#         grouped_words = []\n",
    "#         for key, group in groupby(text_w_word_indicators, key=itemgetter(1)):\n",
    "#             words = [word for word, _ in group]\n",
    "#             grouped_words.append((words, key))\n",
    "#         return grouped_words    \n",
    "    \n",
    "#     def indicators_for_sentence(self, text):\n",
    "#         text = utils.consolidate_double_quotes(text)\n",
    "#         words = text.split()\n",
    "#         num_threads = self.num_threads\n",
    "#         chunk_size = len(words) // num_threads\n",
    "#         chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "#         indicators = [0] * len(words)\n",
    "#         with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "#             results = list(executor.map(self.process_chunk, chunks))\n",
    "#         flat_indicators = [indicator for sublist in results for indicator in sublist]\n",
    "#         quotes_at_word_level = self.combine_text_and_indicators(text,flat_indicators[:len(words)])\n",
    "#         quotes_at_sentence_level = self.group_consecutive_words(quotes_at_word_level)\n",
    "#         return quotes_at_sentence_level\n",
    "        \n",
    "#     def quote_tree(self, fname, text):\n",
    "#         tree_str = \"\"\n",
    "#         # previous_indicator = 0\n",
    "#         for group in self.indicators_for_sentence(text):\n",
    "#             words, indicator = group\n",
    "#             if indicator == 1: \n",
    "#                 quoted_group = ' '.join([f\"{word}\" for word in words])\n",
    "#                 tree_str += f\"(QUOTATION {quoted_group})\"\n",
    "#             else:\n",
    "#                 tree_str += ' '.join(words) + \" \"       \n",
    "#         tree_str = \"(\" + fname + \" \" + tree_str.strip() + \")\"\n",
    "#         return tree_str\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b532cce-6330-4cb6-a9bf-09162328d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quote_indicator = QuotationIndicator(num_threads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7786e218-c901-443c-8379-3e70d785a0a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# text = 'insult. \"What were you doing behind the curtain?\" he asked. \"I was reading.\" \"Show the book.\" I returned to the window and fetched it thence. \"You have no business\"'\n",
    "# test = quote_indicator.quote_tree('TEST', text)\n",
    "# test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca279ec7-99cc-40e3-8408-01f4f3e93d76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# text = 'I never had an idea of replying to it; my care was how to endure the blow which would certainly follow the insult. \"What were you doing behind the curtain?\" he asked. \"I was reading.\" \"Show the book.\" I returned to the window and fetched it thence. \"You have no business to take our books; you are a dependent, mama says; you have no money; your father left you none; you ought to beg, and not to live here with gentlemen’s children like us, and eat the same meals we do, and wear clothes at our mama’s expense. Now, I’ll teach you to rummage my bookshelves: for they _are_ mine; all the house belongs to me, or will do in a few years. Go and stand by the door, out of the way of the mirror and the windows.\" I did so, not at first aware what was his intention; but when I saw him lift and poise the book and stand in act to hurl it, I instinctively started aside with a cry of alarm: not soon enough, however; the volume was flung, it hit me, and I fell, striking my head against the door and cutting it. The cut bled, the pain was sharp: my terror had passed its climax; other feelings succeeded. \"Wicked and cruel boy!\" I said.'\n",
    "# test = quote_indicator.quote_tree('TEST', text)\n",
    "# test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1c4841-35be-4af9-a27e-958de269de33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# desired = '(TEST I never had an idea of replying to it; my care was how to endure the blow which would certainly follow the insult. (QUOTATION \"What were you doing behind the curtain?\") he asked. (QUOTATION \"I was reading.\") (QUOTATION \"Show the book.\") I returned to the window and fetched it thence. (QUOTATION \"You have no business to take our books; you are a dependent, mama says; you have no money; your father left you none; you ought to beg, and not to live here with gentlemen’s children like us, and eat the same meals we do, and wear clothes at our mama’s expense. Now, I’ll teach you to rummage my bookshelves: for they _are_ mine; all the house belongs to me, or will do in a few years. Go and stand by the door, out of the way of the mirror and the windows.\") I did so, not at first aware what was his intention; but when I saw him lift and poise the book and stand in act to hurl it, I instinctively started aside with a cry of alarm: not soon enough, however; the volume was flung, it hit me, and I fell, striking my head against the door and cutting it. The cut bled, the pain was sharp: my terror had passed its climax; other feelings succeeded. (QUOTATION \"Wicked and cruel boy!\")I said.)'\n",
    "# desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9af049-9607-4b66-8ef6-009ee2460210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)\n",
    "quote_indicator = utils.QuotationIndicator(num_threads=1)\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a932591a-81fa-4287-87a6-450cd04a100b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = '/media/daniel/HDD1/AI574/gutenberg/data/raw'\n",
    "selected_works_path = '/media/daniel/HDD1/AI574/Project/selected_works_dive.csv'\n",
    "line_chunk_size = 600\n",
    "txt_all = ''\n",
    "problem_files = []\n",
    "with open(selected_works_path, mode='r', newline='') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for i, row in enumerate(csv_reader):\n",
    "        if i % 500 == 1:\n",
    "            print(i)\n",
    "        target_path = os.path.join('/media/daniel/HDD1/AI574/gutenberg/data/raw',row[0]+'_raw.txt')\n",
    "        lines_as_strings = []\n",
    "        try:\n",
    "            with open(target_path, mode='r', newline='') as file:\n",
    "                lines = file.readlines()\n",
    "                for j in range(0, len(lines), line_chunk_size):\n",
    "                    chunk_lines = lines[j:j+line_chunk_size]\n",
    "                    chunk_as_string = ' '.join(chunk_lines)\n",
    "                    \n",
    "                    chunk_as_string = chunk_as_string.replace('(','<')\n",
    "                    chunk_as_string = chunk_as_string.replace(')','>')                      \n",
    "                    \n",
    "                    chunk_list = chunk_as_string.split()\n",
    "                    chunk_as_string_clean = utils.remove_special_characters(chunk_as_string).lower()\n",
    "                    chunk_for_ner = tokenize_and_convert_to_ids((chunk_as_string))\n",
    "                    chunk_for_ner = tf.reshape(chunk_for_ner, shape=[1, -1])\n",
    "                    \n",
    "                    ner_output = ner_model.predict(chunk_for_ner, verbose=0)\n",
    "                    ner_prediction = np.argmax(ner_output, axis=-1)[0]\n",
    "                    ner_prediction_desc = [MAPPING[i] for i in ner_prediction]\n",
    "                    \n",
    "#                     text_nlp = nlp(chunk_as_string)\n",
    "#                     ner_prediction_desc = [(word.ent_type_ if word.ent_type_ == \"PERSON\" else \"0\") for word in text_nlp]                    \n",
    "#                     ner_prediction_desc = [\"B-PER\" if ent_type == \"PERSON\" else ent_type for ent_type in ner_prediction_desc]\n",
    "                    \n",
    "                    list_words_w_ner_indicators = list(zip(chunk_list, ner_prediction_desc))\n",
    "                    str_words_w_ner_indicators = group_consecutive_words(list_words_w_ner_indicators)\n",
    "                  \n",
    "                    rejoined_words = per_tree(str_words_w_ner_indicators)\n",
    "                    lines_as_strings.append(rejoined_words)\n",
    "            file_lines = ' '.join(lines_as_strings)\n",
    "\n",
    "            text_w_quotation_indicator = quote_indicator.quote_tree(row[0], file_lines)\n",
    "            txt_all += text_w_quotation_indicator\n",
    "        except Exception as e:\n",
    "            problem_files.append(target_path)\n",
    "            print(f\"Error processing {target_path}: {e}\")\n",
    "\n",
    "print(f\"Processed {i} files with {len(problem_files)} errors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc83533e-6fdb-475d-b740-f585b98da995",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt_all[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de76c3d-d025-4a4e-a16f-4a651eacfb1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_text_train = txt_all.split('\\n')\n",
    "with open('jan_eyre_train.txt', mode='w', encoding='utf-8') as file:\n",
    "    for record in raw_text_train:\n",
    "        file.write(record + '\\n')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ccbade-dbad-4a35-a348-20fb37c7f36c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tree import Tree\n",
    "tree = Tree.fromstring(txt_all)\n",
    "# tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2f9f97-873b-4d8a-9f4b-35f93a959913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_ner_from_tree(tree):\n",
    "    names = []\n",
    "    for subtree in tree:\n",
    "        if isinstance(subtree, Tree):  # Check if it is a subtree\n",
    "            if subtree.label() == \"PER\":  # Check for the \"PER\" label\n",
    "                names.append(' '.join(subtree.leaves()).lower())\n",
    "            else:\n",
    "                names.extend(extract_ner_from_tree(subtree))\n",
    "    names = set(names)\n",
    "    return names\n",
    "ner_names = extract_ner_from_tree(tree)\n",
    "ner_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0138b82-cc18-47a7-a7b6-bccbd1afa599",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tree import Tree\n",
    "statement_words = ['said','exclaimed','replied','whispered','uttered','asserted','declared','stated','announced','mentioned','remarked','commented','noted','disclosed','pronounced','muttered','murmured','suggested','reported','articulated','narrated']\n",
    "tree = Tree.fromstring(txt_all)\n",
    "\n",
    "def replace_words(tree, replacement_text=\"_\"):\n",
    "    for idx, subtree in enumerate(tree):\n",
    "        if isinstance(subtree, Tree):\n",
    "            if subtree.label() == \"PER\":\n",
    "                continue \n",
    "            else:\n",
    "                replace_words(subtree, replacement_text)\n",
    "        elif any(word.lower() in ner_names for word in tree[idx].split()):\n",
    "            continue                \n",
    "        elif any(word.lower() in statement_words for word in tree[idx].split()):\n",
    "            continue\n",
    "        else:\n",
    "            tree[idx] = replacement_text\n",
    "\n",
    "replace_words(tree)\n",
    "\n",
    "tree_string_modified = tree.pformat(margin=100000000)\n",
    "# print(tree_string_modified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9b3f3a-771e-43b0-a765-c2aa478404ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d7eea-c762-470e-8071-496c880aba54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffed0bda-80c1-4ff6-ab8b-3e2b1291441f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4af306-80ac-4a79-940b-63c1324e4ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff0b48c-7faf-4818-bbeb-c2eb8fbda518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f8ad1f-37e0-46ff-84ca-a08f03c729f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27c355a-cfca-486b-a829-85ed696d1c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d945313-b569-4b35-8959-bee6a9ae90c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input_file_path = 'jan_eyre_train.txt'\n",
    "# output_file_path = 'jan_eyre_train.txt'\n",
    "\n",
    "# with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "#     lines = file.readlines()\n",
    "\n",
    "# # Remove the first and last character from each line\n",
    "# modified_lines = [line[1:-1] if len(line) > 1 else '' for line in lines]\n",
    "\n",
    "# with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "#     file.writelines(modified_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c67f37-61e5-43a5-8931-3a649fcca9f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = '/media/daniel/HDD1/AI574/gutenberg/data/raw'\n",
    "selected_works_path = '/media/daniel/HDD1/AI574/Project/selected_works_dive2.csv'\n",
    "line_chunk_size = 600\n",
    "txt_all = ''\n",
    "problem_files = []\n",
    "with open(selected_works_path, mode='r', newline='') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for i, row in enumerate(csv_reader):\n",
    "        if i % 500 == 1:\n",
    "            print(i)\n",
    "        target_path = os.path.join('/media/daniel/HDD1/AI574/gutenberg/data/raw',row[0]+'_raw.txt')\n",
    "        lines_as_strings = []\n",
    "        try:\n",
    "            with open(target_path, mode='r', newline='') as file:\n",
    "                lines = file.readlines()\n",
    "                for j in range(0, len(lines), line_chunk_size):\n",
    "                    chunk_lines = lines[j:j+line_chunk_size]\n",
    "                    chunk_as_string = ' '.join(chunk_lines)\n",
    "                    \n",
    "                    chunk_as_string = chunk_as_string.replace('(','<')\n",
    "                    chunk_as_string = chunk_as_string.replace(')','>')                      \n",
    "                    \n",
    "                    chunk_list = chunk_as_string.split()\n",
    "                    chunk_as_string_clean = utils.remove_special_characters(chunk_as_string).lower()\n",
    "                    chunk_for_ner = tokenize_and_convert_to_ids((chunk_as_string))\n",
    "                    chunk_for_ner = tf.reshape(chunk_for_ner, shape=[1, -1])\n",
    "                    ner_output = ner_model.predict(chunk_for_ner, verbose=0)\n",
    "                    ner_prediction = np.argmax(ner_output, axis=-1)[0]\n",
    "                    ner_prediction_desc = [MAPPING[i] for i in ner_prediction]\n",
    "                    list_words_w_ner_indicators = list(zip(chunk_list, ner_prediction_desc))\n",
    "                    str_words_w_ner_indicators = group_consecutive_words(list_words_w_ner_indicators)\n",
    "                  \n",
    "                    rejoined_words = per_tree(str_words_w_ner_indicators)\n",
    "                    lines_as_strings.append(rejoined_words)\n",
    "            file_lines = ' '.join(lines_as_strings)\n",
    "\n",
    "            text_w_quotation_indicator = quote_indicator.quote_tree(row[0], file_lines)\n",
    "            txt_all += text_w_quotation_indicator\n",
    "        except Exception as e:\n",
    "            problem_files.append(target_path)\n",
    "            print(f\"Error processing {target_path}: {e}\")\n",
    "\n",
    "print(f\"Processed {i} files with {len(problem_files)} errors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcec97d-85ba-44e4-80af-0d7a3ca63522",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_text_validate = txt_all.split('\\n')\n",
    "with open('wuthering_heights_validate.txt', mode='w', newline = '\\n', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for record in raw_text_validate:\n",
    "        writer.writerow([record])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4974329-3b9f-4b3d-b41a-dbc152e1243f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb66e091-6e13-4247-949f-23bffb9bf394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5be14e8-ce9d-4fa2-8d4c-ce827854ad27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f43952e-8754-49cd-b756-22d8be9df86b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f07d0-b18a-40dd-930a-43672ea402fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b076f9-daa1-46c0-bb12-3d58385fddd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4ea84c-7829-4c31-8e38-33708c5c703e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bccc93-6f8b-4c78-ba87-90dc82c81700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tree import Tree\n",
    "import svgling\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "samp = SMOTE()\n",
    "from transformers import AutoTokenizer, EarlyStoppingCallback, AutoModelForSequenceClassification\n",
    "from keras.utils import to_categorical\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import EvalPrediction, TrainingArguments, Trainer\n",
    "import torch\n",
    "scaler = GradScaler()\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7476fb4a-96bf-41b7-9466-57bc32a0d98b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from transformers import AutoTokenizer, EarlyStoppingCallback, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "MAX_LEN = 256\n",
    "BATCH = 8\n",
    "METRIC = 'eval_F1'\n",
    "EPOCH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e9bbfe-8608-444c-95c9-2b83fba4fb95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_sentiment_tree(tree):\n",
    "    phrases = []\n",
    "    phrases.append((' '.join(tree.leaves()), tree.label()))\n",
    "    for subtree in tree:\n",
    "        if isinstance(subtree, Tree):\n",
    "            phrases.extend(extract_sentiment_tree(subtree))\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df8a9d5-e80a-49b4-9a34-2533011225b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for raw_text in data['text']:\n",
    "        \n",
    "        tokens = []\n",
    "        seen_tokens = set()\n",
    "        \n",
    "        raw_text = raw_text.strip()\n",
    "        tree = Tree.fromstring(raw_text)    \n",
    "        text_and_labels = extract_sentiment_tree(tree)\n",
    "        for each in text_and_labels:\n",
    "            line_token_str = str(each[0])\n",
    "            if line_token_str not in seen_tokens:\n",
    "                line_token_str = ''.join(ch for ch in line_token_str.lower() if ch not in string.punctuation)\n",
    "                line_token = tokenizer(line_token_str, padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
    "                label_idx = int(each[1])\n",
    "                label_matrix = np.zeros(5)\n",
    "                label_matrix[label_idx] = 1\n",
    "                \n",
    "                all_input_ids.append(line_token['input_ids'])\n",
    "                all_attention_mask.append(line_token['attention_mask'])\n",
    "                all_labels.append(label_matrix.tolist())\n",
    "                seen_tokens.add(line_token_str)\n",
    "                \n",
    "    target_len = len(data[\"text\"])\n",
    "    \n",
    "    all_input_ids = all_input_ids[:target_len] + [[0] * MAX_LEN] * (target_len - len(all_input_ids))\n",
    "    all_attention_mask = all_attention_mask[:target_len] + [[0] * MAX_LEN] * (target_len - len(all_attention_mask))\n",
    "    all_labels = all_labels[:target_len] + [[0, 0, 0, 0, 0]] * (target_len - len(all_labels))\n",
    "    \n",
    "    return {\"input_ids\": all_input_ids, \"attention_mask\": all_attention_mask, \"labels\": all_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934ebcb0-3ec7-4ccf-acb1-89ad87778e7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('text', data_files={'train': 'jan_eyre_train.txt', 'validation': 'wuthering_heights_validate.txt'})\n",
    "train_dataset = preprocess(dataset['train'])\n",
    "validation_dataset = preprocess(dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90680cea-ed7b-47c6-b7b6-da34e1b9347b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict(train_dataset)\n",
    "validation_dataset= Dataset.from_dict(validation_dataset)\n",
    "\n",
    "encoded_dataset = DatasetDict({\n",
    "    \"train\":train_dataset,\n",
    "    \"validation\": validation_dataset\n",
    "})\n",
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf5a587-0c02-48e9-bb9c-e0aaaef5ea77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    encoded_dataset,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=True,\n",
    "    num_workers=8,  # adjust based on your system's capabilities\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9291d3a5-e909-49e6-b5c8-e0ccad6a2e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd239ce7-d908-427f-a792-b769a0a1674b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example = encoded_dataset['train'][0]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11999ae1-a098-46b4-a381-4fb34f5b459b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = ['Root', 'Quotation', 'Per', 'None1', 'None2']\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8565de-91f0-461b-b1f8-9ab507cdbb9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           hidden_dropout_prob=0.5, #overfitting issue - overrode dropout default 0.1 to 0.2\n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e1665-c978-4a3a-88c0-88ef5574bd6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"bert-finetuned-sem_eval-english\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=BATCH,\n",
    "    per_device_eval_batch_size=BATCH,\n",
    "    num_train_epochs=EPOCH,\n",
    "    weight_decay=0.02, #Initial value 0.01 resulted in overfitting - tried 0.02 but did not correct\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=METRIC,\n",
    "    save_total_limit = 3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b052355-69f8-4cb9-b0d6-9211ad88fec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    metrics = {'eval_F1': f1_micro_average,\n",
    "               'eval_roc_auc': roc_auc,\n",
    "               'eval_accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca3c6ab-6f91-41e3-b051-a788076cbb80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bc847b-cce7-4b1b-b51d-dc31f5d86cf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(encoded_dataset['train']['input_ids'][0].unsqueeze(0).shape)\n",
    "print(encoded_dataset['train']['attention_mask'][0].unsqueeze(0).shape)\n",
    "print(encoded_dataset['train']['labels'][0].unsqueeze(0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996dcf63-aa18-48cb-8035-15d430f8e1e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = model(\n",
    "    input_ids=encoded_dataset['train']['input_ids'][0].unsqueeze(0),\n",
    "    attention_mask=encoded_dataset['train']['attention_mask'][0].unsqueeze(0),\n",
    "    labels=encoded_dataset['train']['labels'][0].unsqueeze(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa062f-a6a4-4237-9fd4-d9aeebe99629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience = 2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da17476-b4b6-40d9-8893-40def8fc237f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ddd0c7-8a1e-45a9-9013-c52438401c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f650a0fd-88d2-4574-b9d0-6a27d638574c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e607a9a6-8436-4e4c-b1dd-2caa7316cf81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241b7635-a208-4e5c-a742-04a3fa56e0d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_dir = '/media/daniel/HDD1/AI574/gutenberg/data/raw'\n",
    "# selected_works_path = '/media/daniel/HDD1/AI574/Project/selected_works_idx.csv'\n",
    "# import csv\n",
    "# import re\n",
    "# txt_all = ''\n",
    "# problem_files = []\n",
    "# line_chunk_size = 10\n",
    "# with open(selected_works_path, mode='r', newline='') as file:\n",
    "#     csv_reader = csv.reader(file)\n",
    "#     i = 0\n",
    "#     for row in csv_reader:\n",
    "#         if i % 500 == 1:\n",
    "#             print(i)\n",
    "#         if i == 2:\n",
    "#             break\n",
    "#         target_path = os.path.join('/media/daniel/HDD1/AI574/gutenberg/data/raw',row[0]+'_raw.txt')\n",
    "#         lines_as_strings = []\n",
    "#         with open(target_path, mode='r', newline='') as file:\n",
    "#             lines = file.readlines()\n",
    "#             j = 0\n",
    "#             for k in range(0, len(lines), line_chunk_size):\n",
    "#                 j += 1\n",
    "#                 if j == 30:\n",
    "#                     break\n",
    "#                 chunk_lines = lines[j:j+line_chunk_size]\n",
    "#                 # print(chunk_lines)\n",
    "                \n",
    "#                 line_string = ' '.join(chunk_lines)\n",
    "                \n",
    "#                 line_list = line_string.split()\n",
    "#                 line_for_ner = tokenize_and_convert_to_ids((line_string))\n",
    "#                 line_for_ner = tf.reshape(line_for_ner, shape=[1, -1])\n",
    "#                 ner_output = ner_model.predict(line_for_ner, verbose=0)\n",
    "#                 ner_prediction = np.argmax(ner_output, axis=-1)[0]\n",
    "#                 ner_prediction = [MAPPING[i] for i in ner_prediction]\n",
    "#                 list_words_w_ner_indicators = list(zip(line_list, ner_prediction))\n",
    "#                 words_w_ner_indicators = group_consecutive_words(list_words_w_ner_indicators)\n",
    "#                 rejoined_words = per_tree(words_w_ner_indicators)\n",
    "#                 lines_as_strings.append(rejoined_words)\n",
    "                \n",
    "# #              \n",
    "                \n",
    "#         file_lines = ' '.join(lines_as_strings)\n",
    "#         text_w_quotation_indicator = quote_indicator.quote_tree(row[0], file_lines)\n",
    "#         i += 1\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fabcd6-71b7-49d0-b7ef-fa4bde3c6e78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# txt_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdc2b3f-9445-4584-979e-be832ff848b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_dir = '/media/daniel/HDD1/AI574/gutenberg/data/raw'\n",
    "# # selected_works_path = '/media/daniel/HDD1/AI574/Project/selected_works_idx.csv'\n",
    "# selected_works_path = '/media/daniel/HDD1/AI574/Project/selected_works_dive.csv'\n",
    "# import csv\n",
    "# import re\n",
    "# txt_all = ''\n",
    "# problem_files = []\n",
    "# with open(selected_works_path, mode='r', newline='') as file:\n",
    "#     csv_reader = csv.reader(file)\n",
    "#     i = 0\n",
    "#     for row in csv_reader:\n",
    "#         if i % 500 == 1:\n",
    "#             print(i)\n",
    "#         if i == 5:\n",
    "#             break\n",
    "#         target_path = os.path.join('/media/daniel/HDD1/AI574/gutenberg/data/raw',row[0]+'_raw.txt')\n",
    "#         # print(target_path)\n",
    "#         lines_as_strings = []\n",
    "#         # try:\n",
    "#         with open(target_path, mode='r', newline='') as file:\n",
    "#             # csv_reader = csv.reader(file)\n",
    "#             lines = file.readlines()\n",
    "#             j = 0\n",
    "#             for line in lines:\n",
    "#                 j += 1\n",
    "#                 if j == 24:\n",
    "#                     break\n",
    "#                 line_string = ' '.join(line)\n",
    "#                 line_list = line.split()\n",
    "#                 line_for_ner = tokenize_and_convert_to_ids((line))\n",
    "#                 line_for_ner = tf.reshape(line_for_ner, shape=[1, -1])\n",
    "#                 ner_output = ner_model.predict(line_for_ner, verbose=0)\n",
    "#                 ner_prediction = np.argmax(ner_output, axis=-1)[0]\n",
    "#                 ner_prediction = [MAPPING[i] for i in ner_prediction]\n",
    "#                 words_w_ner_indicators = list(zip(line_list, ner_prediction))\n",
    "#                 words_w_ner_indicators = group_consecutive_words(words_w_ner_indicators)                 \n",
    "#                 rejoined_words = per_tree(words_w_ner_indicators)\n",
    "#                 lines_as_strings.append(rejoined_words)\n",
    "#         file_lines = ' '.join(lines_as_strings)\n",
    "\n",
    "#         # all_lines = utils.remove_special_characters(all_lines, remove_digits=True).lower()    \n",
    "#         # all_lines = all_lines[1:100]\n",
    "#         text_w_quotation_indicator = quote_indicator.quote_tree(row[0], file_lines)\n",
    "        \n",
    "\n",
    "#         i += 1\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b046a5df-63e9-4690-9a98-0ab086debc93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# text_w_quotation_indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be40362-0b30-42a8-9946-84b419be2b84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = '(' + text_w_quotation_indicator[950:1250] + ')'\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb45be8b-f49c-4106-acb1-66cbc580271b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tree import Tree\n",
    "tree = Tree.fromstring(test)\n",
    "# tree.pretty_print()\n",
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be906b6-3116-49e0-a225-32251bfa42e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "records = txt_all.split('\\n')\n",
    "\n",
    "with open('gutenberg_scored.txt', mode='w', newline = '\\n', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for record in records:\n",
    "        writer.writerow([record])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6d8ca0-7eb3-4eb0-babb-89506cd65679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347c5781-e4ae-4139-9c8e-c91b70a7ae8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f64e0-a5aa-472f-a2f6-23ff36e9a3f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cur_txt = cur_txt[:20000]\n",
    "orig_words = cur_txt.split()\n",
    "# print(cur_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787a6825-fb94-4b7d-a876-9a550e57a2eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cur_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd93a1d-cf99-4cc2-bee5-635252bf8e00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_by_sentence = cur_txt.split('.')\n",
    "split_by_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde16634-04ae-4849-9353-542f32ef2a90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_input = tokenize_and_convert_to_ids(cur_txt)\n",
    "sample_input = tf.reshape(sample_input, shape=[1, -1])\n",
    "# print(sample_input)\n",
    "\n",
    "output = ner_model.predict(sample_input)\n",
    "prediction = np.argmax(output, axis=-1)[0]\n",
    "prediction = [MAPPING[i] for i in prediction]\n",
    "# print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ec07e-5260-4b4e-9de5-2d3aa9830b22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "positions = [index for index, value in enumerate(prediction) if value != 'O']\n",
    "NER_words = list(set([orig_words[index] for index in positions]))\n",
    "NER_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ef394-8710-46e8-9c7d-20f00192285f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.named_persons_w_spacy(cur_txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
